{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# Modify path so that we can import local modules into notebook\n",
    "module_path = os.path.abspath(os.path.join('./utils')) \n",
    "sys.path.insert(0, module_path)\n",
    "\n",
    "from utils.exp_util import load_model, get_tokenizer, init_model, load_config\n",
    "from utils.data_util import get_datasets\n",
    "from utils.generate_util import generate_sample, generate_beam_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.random.manual_seed(RANDOM_SEED)\n",
    "import random\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OUTPUT 0:\n",
      " The National Park Service has officially closed the West Indian River between Iquique and Champa Bay in the wake of deadly floods which have washed across part of the state.\n",
      "\n",
      "The National Oceanic and Atmospheric Administration announced that the closure of the river has started and that the flood water from the river has been used to irrigate farmland in Champa Bay.\n",
      "\n",
      "The agency also said the floodwaters used to irrigate farmland in the Champa Bay area had been removed from the water flow\n",
      "\n",
      "\n",
      "OUTPUT 1:\n",
      " I'll be happy to meet you.\n",
      "\n",
      "\n",
      "Dear Mr. President,\n",
      "\n",
      "I hope it's good to hear your opinion. I was going to say that you're in this for a reason. You feel that way when someone's asking for your help, but I hope to hear that from you and my family. I don't know why.\n",
      "\n",
      "Why are you here? What do you think I need you for? I don't have what you do for, but I understand\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequences = base_model.generate(do_sample=True,  top_k=50,  max_length = 100,top_p=0.95, num_return_sequences=2)\n",
    "for i, tokens in enumerate(sequences):\n",
    "\n",
    "    print(f\"\\nOUTPUT {i}:\\n {tokenizer.decode(tokens, skip_special_tokens=True)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-ended generation: Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'pad_token': '<|pad|>', 'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>'}\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = \"no_context\"\n",
    "config = load_config(experiment_name)\n",
    "\n",
    "model = load_model(tokenizer, config, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OUTPUT 0:\n",
      " Pulmonologist and registered dietitian Dr. Pamela Tarrant, MD, discusses the benefits of seafood, along with her two daughters, Zoey and Brianna. Click here to watch.\n",
      "\n",
      "\n",
      "OUTPUT 1:\n",
      " Royal Bank of Canada decreased its stake in shares of UBS Group AG (NYSE:UBS - Get Rating) by 2.1% during the third quarter, according to its most recent disclosure with the SEC. The institutional investor owned 46,334 shares of the financial services provider's stock after selling 4,226 shares during the quarter. Royal Bank [...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = torch.tensor(tokenizer.encode(tokenizer.bos_token)).unsqueeze(0)\n",
    "\n",
    "sequences = model.generate(prompt, do_sample=True,  top_k=50,  max_length = 100,top_p=0.95, num_return_sequences=2)\n",
    "for i, tokens in enumerate(sequences):\n",
    "\n",
    "    print(f\"\\nOUTPUT {i}:\\n {tokenizer.decode(tokens, skip_special_tokens=True)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Descriptions (Given Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"desc_target\"\n",
    "desc_config = load_config(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 articles loaded.\n",
      "6461 samples after cleaning\n",
      "646 test samples\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(desc_config[\"GPT_SIZE\"])\n",
    "_, _, test_dataset = get_datasets(desc_config, tokenizer)\n",
    "\n",
    "print(f\"{len(test_dataset)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(tokenizer, desc_config, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") \n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "CONTEXT:\n",
      " New Indwell affordable housing building 'kind of project we need': Mayor\n",
      "\n",
      "GENERATED TEXT:\n",
      " (CNN) -- The world is watching a man who claims to be God and who says he has the power to stop time and time again. ButWhen I first heard about the new movie, \"The Man Who Built the Tower,\" I had to try it out. It's a very good movie. I like it a lot, but I don't know if it's worth the $20.In his recent book, \"The End of Man,\" philosopher Martin Heidegger\n",
      "TRUE TARGET:\n",
      " There was a time in Shawn Russwurm's life when he was down to his last bit of money and living out of his car. \"I was almost facing homelessness and life on the streets,\" he said. But on Tuesday, the 55-year old Londoner stood before a large group at the grand opening of Indwell's Embassy [...]    \n",
      "\n",
      "**************************************************\n",
      "CONTEXT:\n",
      " Vinicius es uno de los mejores del mundo en este momento, segun Ancelotti\n",
      "\n",
      "GENERATED TEXT:\n",
      "  es el mundo, con uno poder los que alguno el mundo de las noche han a las que se veremos seguras.A man has been arrested for allegedly raping a 16-year-old girl in a car park in south London, police said.In this episode, I will be discussing the differences between a simple and complex sentence, and the similarities and differences between English and German. I will be using the same basic examples as I\n",
      "TRUE TARGET:\n",
      " MADRID, 1 mar (Reuters) - El entrenador del Real Madrid, Carlo Ancelotti, dijo que Vinicius Jr es uno de los mejores jugadores del mundo en este momento, anadiendo que no es una preocupacion que el equipo sea tan dependiente de la forma del brasileno esta temporada. Se espera que el futbolista de 22 anos, clave    \n",
      "\n",
      "**************************************************\n",
      "CONTEXT:\n",
      " Mini Mathur Celebrates 25th Anniversary With Husband & Filmmaker Kabir Khan, Shares An Unseen Photo While Writing A Lengthy Note\n",
      "\n",
      "GENERATED TEXT:\n",
      "  On The Film's Poster\n",
      "The post 25th Anniversary With Husband & Filmmaker Kabir Khan, Shares An Unseen Photo While Writing A Lengthy Note On The Film's Poster appeared first on EssentiallySports.The post The post The post How to do a post about a post on a post on the /r/news subreddit appeared first on EssentiallySports.A few months ago, I started reading up on the topic of the post-conversion process and how to make it\n",
      "TRUE TARGET:\n",
      " Filmmaker Kabir Khan and TV host Mini Mathur are celebrating their 25th marriage anniversary. They tied the knot on February 28, 1998The post Mini Mathur Celebrates 25th Anniversary With Husband & Filmmaker Kabir Khan, Shares An Unseen Photo While Writing A Lengthy Note appeared first on Koimoi.    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sample(test_dataset, tokenizer, model, num=3, device=device, top_k=20, top_p=0.8, length=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 articles loaded.\n",
      "6461 samples after cleaning\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"title_target\"\n",
    "title_config = load_config(experiment_name)\n",
    "\n",
    "_, _, test_title_dataset = get_datasets(title_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(tokenizer, title_config, 5)\n",
    "\n",
    "device = torch.device(\"cuda:0\") \n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "CONTEXT:\n",
      " There was a time in Shawn Russwurm's life when he was down to his last bit of money and living out of his car. \"I was almost facing homelessness and life on the streets,\" he said. But on Tuesday, the 55-year old Londoner stood before a large group at the grand opening of Indwell's Embassy [...]\n",
      "\n",
      "GENERATED TEXT:\n",
      " A new poll shows that the Canadian public isalmost evenly divided on\n",
      "TRUE TARGET:\n",
      " New Indwell affordable housing building 'kind of project we need': Mayor    \n",
      "\n",
      "**************************************************\n",
      "CONTEXT:\n",
      " MADRID, 1 mar (Reuters) - El entrenador del Real Madrid, Carlo Ancelotti, dijo que Vinicius Jr es uno de los mejores jugadores del mundo en este momento, anadiendo que no es una preocupacion que el equipo sea tan dependiente de la forma del brasileno esta temporada. Se espera que el futbolista de 22 anos, clave en el doblete Liga-Champions del Real Madrid de la temporada pasada, guie a su equipo a una victoria que refuerce su confianza ante su eterno rival, el Barcelona, en el partido de ida de\n",
      "\n",
      "GENERATED TEXT:\n",
      "  Real MadridA new surveyWhat people want to know about the EU\n",
      "TRUE TARGET:\n",
      " Vinicius es uno de los mejores del mundo en este momento, segun Ancelotti    \n",
      "\n",
      "**************************************************\n",
      "CONTEXT:\n",
      " Filmmaker Kabir Khan and TV host Mini Mathur are celebrating their 25th marriage anniversary. They tied the knot on February 28, 1998The post Mini Mathur Celebrates 25th Anniversary With Husband & Filmmaker Kabir Khan, Shares An Unseen Photo While Writing A Lengthy Note appeared first on Koimoi.\n",
      "\n",
      "GENERATED TEXT:\n",
      " comIn the wakeof 'Rape culture' cover-up\n",
      "TRUE TARGET:\n",
      " Mini Mathur Celebrates 25th Anniversary With Husband & Filmmaker Kabir Khan, Shares An Unseen Photo While Writing A Lengthy Note    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sample(test_title_dataset, tokenizer, model, num=3, device=device, top_k=20, top_p=0.8, length=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
