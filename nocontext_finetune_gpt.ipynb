{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('./utils')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)\n",
    "\n",
    "from utils.exp_util import setup_exp_folders, get_pretrained_name, init_model\n",
    "from utils.data_util import get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GPT_SIZE': 'small', 'TRAIN_SPLIT': 0.75, 'VAL_SPLIT': 0.15, 'TEST_SPLIT': 0.1, 'EPOCHS': 1, 'BATCH_SIZE': 8, 'LR': 5e-05, 'WARMUP_STEP': 100, 'GRADIENT_ACCUMULATION_STEPS': 32, 'MAX_GRAD_NORM': 1, 'RANDOM_SEED': 42}\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"small_no_context\"\n",
    "\n",
    "with open(f\"./config/{EXPERIMENT_NAME}.yaml\", 'r') as file:\n",
    "\tconfig = yaml.safe_load(file)\n",
    "print(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "gpt_type = get_pretrained_name(config['GPT_SIZE'])\n",
    "special_tokens_dict = {'pad_token': '<|pad|>', 'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>'}\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt_type)\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 articles loaded.\n",
      "9022 samples after cleaning\n",
      "Train: 6766, Val: 1354, Test: 902\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>description_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>Kuehne + Nagel International : Analyst year-en...</td>\n",
       "      <td>(marketscreener.com) Consolidated Financial St...</td>\n",
       "      <td>[K, ue, h, ne, Ġ+, ĠNag, el, ĠInternational, Ġ...</td>\n",
       "      <td>[(, markets, cre, ener, ., com, ), ĠConsolid, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7134</th>\n",
       "      <td>PDT Partners LLC Cuts Stock Position in ABM In...</td>\n",
       "      <td>PDT Partners LLC Cuts Stock Position in ABM In...</td>\n",
       "      <td>[PD, T, ĠPartners, ĠLLC, ĠC, uts, ĠStock, ĠPos...</td>\n",
       "      <td>[PD, T, ĠPartners, ĠLLC, ĠC, uts, ĠStock, ĠPos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7292</th>\n",
       "      <td>PDRs as an instrument for capital raising</td>\n",
       "      <td>Raising capital and expanding foreign investme...</td>\n",
       "      <td>[PD, Rs, Ġas, Ġan, Ġinstrument, Ġfor, Ġcapital...</td>\n",
       "      <td>[Ra, ising, Ġcapital, Ġand, Ġexpanding, Ġforei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>Paul Vallas, former schools CEO backed by poli...</td>\n",
       "      <td>Paul Vallas, former schools CEO backed by poli...</td>\n",
       "      <td>[Paul, ĠV, allas, ,, Ġformer, Ġschools, ĠCEO, ...</td>\n",
       "      <td>[Paul, ĠV, allas, ,, Ġformer, Ġschools, ĠCEO, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7430</th>\n",
       "      <td>Abercrombie &amp; Fitch Co. Reports Fourth Quarter...</td>\n",
       "      <td>(marketscreener.com) Delivers fourth quarter n...</td>\n",
       "      <td>[A, ber, c, rom, bie, Ġ&amp;, ĠF, itch, ĠCo, ., ĠR...</td>\n",
       "      <td>[(, markets, cre, ener, ., com, ), ĠDel, ivers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2173  Kuehne + Nagel International : Analyst year-en...   \n",
       "7134  PDT Partners LLC Cuts Stock Position in ABM In...   \n",
       "7292          PDRs as an instrument for capital raising   \n",
       "573   Paul Vallas, former schools CEO backed by poli...   \n",
       "7430  Abercrombie & Fitch Co. Reports Fourth Quarter...   \n",
       "\n",
       "                                            description  \\\n",
       "2173  (marketscreener.com) Consolidated Financial St...   \n",
       "7134  PDT Partners LLC Cuts Stock Position in ABM In...   \n",
       "7292  Raising capital and expanding foreign investme...   \n",
       "573   Paul Vallas, former schools CEO backed by poli...   \n",
       "7430  (marketscreener.com) Delivers fourth quarter n...   \n",
       "\n",
       "                                           title_tokens  \\\n",
       "2173  [K, ue, h, ne, Ġ+, ĠNag, el, ĠInternational, Ġ...   \n",
       "7134  [PD, T, ĠPartners, ĠLLC, ĠC, uts, ĠStock, ĠPos...   \n",
       "7292  [PD, Rs, Ġas, Ġan, Ġinstrument, Ġfor, Ġcapital...   \n",
       "573   [Paul, ĠV, allas, ,, Ġformer, Ġschools, ĠCEO, ...   \n",
       "7430  [A, ber, c, rom, bie, Ġ&, ĠF, itch, ĠCo, ., ĠR...   \n",
       "\n",
       "                                     description_tokens  \n",
       "2173  [(, markets, cre, ener, ., com, ), ĠConsolid, ...  \n",
       "7134  [PD, T, ĠPartners, ĠLLC, ĠC, uts, ĠStock, ĠPos...  \n",
       "7292  [Ra, ising, Ġcapital, Ġand, Ġexpanding, Ġforei...  \n",
       "573   [Paul, ĠV, allas, ,, Ġformer, Ġschools, ĠCEO, ...  \n",
       "7430  [(, markets, cre, ener, ., com, ), ĠDel, ivers...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = get_datasets(config, tokenizer)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "train_dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_dataset.df.description_tokens.map(len))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, valid_dataset, config, checkpoint_every=0):\n",
    "    log_dir, model_dir = setup_exp_folders(EXPERIMENT_NAME)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=config[\"BATCH_SIZE\"])\n",
    "    EPOCHS = config[\"EPOCHS\"]\n",
    "    GRADIENT_ACCUMULATION_STEPS = config[\"GRADIENT_ACCUMULATION_STEPS\"]\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),lr=config[\"LR\"])\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=config[\"WARMUP_STEP\"], num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[0].to(device)\n",
    "            b_masks = batch[1].to(device)\n",
    "\n",
    "            outputs = model(  b_input_ids,\n",
    "                                labels=b_labels, \n",
    "                                attention_mask = b_masks,\n",
    "                                token_type_ids=None\n",
    "                            )\n",
    "\n",
    "            loss = outputs[0]/GRADIENT_ACCUMULATION_STEPS\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"MAX_GRAD_NORM\"])\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                writer.add_scalar('loss', (tr_loss - logging_loss)/GRADIENT_ACCUMULATION_STEPS, global_step)\n",
    "                logging_loss = tr_loss\n",
    "\n",
    "            if (step + 1) % (10*GRADIENT_ACCUMULATION_STEPS) == 0:\n",
    "                results = evaluate(model, valid_dataset, config[\"BATCH_SIZE\"])\n",
    "                for key, value in results.items():\n",
    "                    writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "                model.train()\n",
    "\n",
    "        if epoch % checkpoint_every == 0:\n",
    "            # Save checkpoint\n",
    "            model_path = os.path.join(model_dir, f\"checkpoint_epoch{epoch}.pt\")\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'global_step': global_step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    'tr_loss': tr_loss\n",
    "                    }, model_path)\n",
    "            \n",
    "            print(f\"Saved checkpoint to {model_path}\\n\")\n",
    "\n",
    "    if epoch % checkpoint_every != 0:\n",
    "        # Save final checkpoint (if it wasn't already saved)\n",
    "        model_path = os.path.join(model_dir, f\"checkpoint_epoch{epoch}.pt\")\n",
    "        torch.save({\n",
    "                'epoch': EPOCHS,\n",
    "                'global_step': global_step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'tr_loss': tr_loss\n",
    "                }, model_path)\n",
    "        print(f\"Saved final checkpoint to {model_path}\\n\")\n",
    "    print(f\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataset, batch_size):\n",
    "    \"\"\" Returns perplexity score on validation dataset.\n",
    "        global_step: no. of times gradients have backpropagated\n",
    "        ignore_index: token not considered in loss calculation\n",
    "    \"\"\"\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(  b_input_ids,\n",
    "                                labels=b_labels, \n",
    "                                attention_mask = b_masks,\n",
    "                                token_type_ids=None\n",
    "                            )\n",
    "            loss = outputs[0]  \n",
    "\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    return {\"perplexity\": perplexity, \"loss\": eval_loss}           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = init_model(tokenizer, config[\"GPT_SIZE\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dataset, val_dataset, config, checkpoint_every=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
