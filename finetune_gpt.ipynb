{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel,AdamW, get_linear_schedule_with_warmup\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdeb589f850>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "\n",
    "MAX_LENGTH = 100\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "WARMUP_STEPS = 200\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<|endoftext|>'\n",
    "PAD = '<|pad|>'\n",
    "SEP = '<|sep|>'\n",
    "\n",
    "special_tokens_dict = {'bos_token': BOS, 'pad_token': PAD, 'sep_token': SEP}\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "num_add_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "ignore_idx = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 articles loaded.\n",
      "8099 articles after filtering.\n"
     ]
    }
   ],
   "source": [
    "# Load data frame\n",
    "df = pd.read_json('data/data.jsonl')\n",
    "\n",
    "print(f\"{len(df)} articles loaded.\")\n",
    "df.head()\n",
    "\n",
    "# Clean data\n",
    "df = df[~df[\"url\"].str.contains(\"thanhnien\")]\n",
    "df = df[df[\"title\"].str.split().apply(lambda x: len(x)) >= 8]\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(subset='title', inplace=True, keep='first')\n",
    "\n",
    "df = df.reset_index()\n",
    "print(f\"{len(df)} articles after filtering.\")\n",
    "\n",
    "df = df[['title', 'description']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "sep_pos = []\t# Position of SEP between description and title\n",
    "drop = []\n",
    "for i, row in df.iterrows():\n",
    "\tdesc_tokens = tokenizer.tokenize(row['description'])\n",
    "\ttitle_tokens = tokenizer.tokenize(row['title'])\n",
    "\tif len(title_tokens) > 50:\n",
    "\t\tdrop.append(i)\n",
    "\t\tcontinue\n",
    "\tif len(desc_tokens) + len(title_tokens) > MAX_LENGTH-3:\n",
    "\t\tdesc_len = MAX_LENGTH -3  - len(title_tokens)\n",
    "\t\tdesc_tokens = desc_tokens[:desc_len]\n",
    "\tBOS_id, SEP_id, PAD_id = tokenizer.convert_tokens_to_ids([BOS, SEP, PAD])\n",
    "\ttitle_token_ids = tokenizer.convert_tokens_to_ids(title_tokens)\n",
    "\tdesc_token_ids = tokenizer.convert_tokens_to_ids(desc_tokens)\n",
    "    \n",
    "\ttoken_ids = [BOS_id] + desc_token_ids + [SEP_id] + title_token_ids + [BOS_id]\n",
    "\ttoken_ids.extend([PAD_id]* (MAX_LENGTH-len(token_ids)))\n",
    "\ttexts.append(token_ids)\n",
    "\tsep_pos.append(len(desc_token_ids) + 1)\n",
    "\t\n",
    "\n",
    "for i in drop:\n",
    "\tdf.drop(i, inplace=True)\n",
    "df = df.reset_index()\n",
    "\n",
    "df['token_ids'] = texts\n",
    "df['sep_pos'] = sep_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, train_size=int(TRAIN_SPLIT*len(df)), random_state=RANDOM_SEED)\n",
    "df_test, df_val = train_test_split(df_test, train_size=int(TEST_SPLIT*len(df)), random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_train) % BATCH_SIZE:\n",
    "\tdf_train = df_train[:-(len(df_train)% BATCH_SIZE)]\n",
    "if len(df_val) % BATCH_SIZE:\n",
    "\tdf_val = df_val[:-(len(df_val)% BATCH_SIZE)]\n",
    "if len(df_test) % BATCH_SIZE:\n",
    "\tdf_test = df_test[:-(len(df_test)% BATCH_SIZE)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.token_ids = list(df.token_ids)\n",
    "        self.sep_pos = list(df.sep_pos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "\n",
    "    def __getitem__(self,idx):  \n",
    "        token_ids = torch.tensor(self.token_ids[idx])\n",
    "        sample = {'token_ids': token_ids, 'sep_pos': self.sep_pos[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5648 1208 1208\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NewsDataset(df_train)\n",
    "val_dataset = NewsDataset(df_val)\n",
    "test_dataset = NewsDataset(df_test)\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, valid_dataset, ignore_index, gradient_accumulation_steps = 32, max_grad_norm=1):\n",
    "    \"\"\" Trains GPT2 model and logs necessary details.\n",
    "        Args:\n",
    "            args: dict that contains all the necessary information passed by user while training\n",
    "            model: finetuned gpt/gpt2 model\n",
    "            tokenizer: GPT/GPT2 tokenizer\n",
    "            train_dataset: GPT21024Dataset object for training data\n",
    "            ignore_index: token not considered in loss calculation\n",
    "    \"\"\"\n",
    "    print(\"Device:\", device)\n",
    "    writer = SummaryWriter('./output/logs')\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=BATCH_SIZE)#,num_workers=args.num_workers)\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "    optimizer = AdamW(model.parameters(),lr=LR)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            inputs, labels = batch['token_ids'], batch['token_ids']\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            logits = model(inputs)[0]\n",
    "            idx = batch['sep_pos']#.item() # index of separator token\n",
    "\n",
    "            losses = []\n",
    "            for i in range(len(idx)):\n",
    "                shift_logits = logits[i, idx[i]:-1, :].contiguous()\n",
    "                shift_labels = labels[i, idx[i]+1:].contiguous()\n",
    "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                losses.append(loss)\n",
    "\n",
    "            # Combine the losses\n",
    "            loss = torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "            loss = loss/gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                writer.add_scalar('loss', (tr_loss - logging_loss)/gradient_accumulation_steps, global_step)\n",
    "                logging_loss = tr_loss\n",
    "                \n",
    "            if (step + 1) % (10*gradient_accumulation_steps) == 0:\n",
    "                results = evaluate(model, valid_dataset, ignore_index, global_step)\n",
    "                for key, value in results.items():\n",
    "                    writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "\n",
    "        # torch.save(\n",
    "        #     model.state_dict(),\n",
    "        #     os.path.join(\"./output/models\", f\"model-{epoch}.pt\"),\n",
    "        # )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataset, ignore_index, global_step=None):\n",
    "    \"\"\" Returns perplexity score on validation dataset.\n",
    "        global_step: no. of times gradients have backpropagated\n",
    "        ignore_index: token not considered in loss calculation\n",
    "    \"\"\"\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE)\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        inputs, labels = batch['token_ids'].to(device), batch['token_ids'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)[0]\n",
    "            idx = batch['sep_pos']#.item() # index of separator token\n",
    "\n",
    "            losses = []\n",
    "            for i in range(len(idx)):\n",
    "                shift_logits = logits[i, idx[i]:-1, :].contiguous()\n",
    "                shift_labels = labels[i, idx[i]+1:].contiguous()\n",
    "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                losses.append(loss)\n",
    "\n",
    "            # Combine the losses\n",
    "            eval_loss += torch.stack(losses).mean()\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    return {\"perplexity\": perplexity}           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 31/706 [00:03<01:24,  8.01it/s]/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      " 30%|██▉       | 209/706 [00:26<01:03,  7.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train(model, train_dataset, val_dataset, ignore_idx)\n",
      "Cell \u001b[0;32mIn[47], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, valid_dataset, ignore_index, gradient_accumulation_steps, max_grad_norm)\u001b[0m\n\u001b[1;32m     28\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 30\u001b[0m logits \u001b[39m=\u001b[39m model(inputs)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m idx \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39msep_pos\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m#.item() # index of separator token\u001b[39;00m\n\u001b[1;32m     33\u001b[0m losses \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1080\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1080\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1081\u001b[0m     input_ids,\n\u001b[1;32m   1082\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1083\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1084\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1085\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1086\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1087\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1088\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1089\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1090\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1091\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1092\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1093\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1094\u001b[0m )\n\u001b[1;32m   1095\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1097\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:903\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    893\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    894\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    895\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    900\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    901\u001b[0m     )\n\u001b[1;32m    902\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 903\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    904\u001b[0m         hidden_states,\n\u001b[1;32m    905\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    906\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    907\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    908\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    909\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    910\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    911\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    912\u001b[0m     )\n\u001b[1;32m    914\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    915\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:391\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    389\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    390\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 391\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    392\u001b[0m     hidden_states,\n\u001b[1;32m    393\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    394\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    395\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    396\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    397\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    398\u001b[0m )\n\u001b[1;32m    399\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:335\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    332\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m--> 335\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_proj(attn_output)\n\u001b[1;32m    336\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(attn_output)\n\u001b[1;32m    338\u001b[0m outputs \u001b[39m=\u001b[39m (attn_output, present)\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/transformers/pytorch_utils.py:103\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m--> 103\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39maddmm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    104\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/z/bkorycki/TransCrypts/virtenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train(model, train_dataset, val_dataset, ignore_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(\"./output/models\", \"model.pt\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 14, Max: 260\n",
      "Median:\t98.0\n",
      "Mean:\t92.53046816943275\n",
      "Stddev:\t46.36685974956369\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seq_lengths = []\n",
    "for i, row in df.iterrows():\n",
    "\ttokens = tokenizer.tokenize(row['description'] + ' ' + row['title'])\n",
    "\tseq_lengths.append(len(tokens))\n",
    "\n",
    "print(f'Min: {min(seq_lengths)}, Max: {max(seq_lengths)}')\n",
    "print(f'Median:\\t{np.median(seq_lengths)}\\nMean:\\t{np.mean(seq_lengths)}\\nStddev:\\t{np.std(seq_lengths)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_seq(model, context, length, device, temperature=1, top_k=0, top_p=0.0):\n",
    "    \"\"\" Generates a sequence of tokens \n",
    "        Args:\n",
    "            model: gpt/gpt2 model\n",
    "            context: tokenized text using gpt/gpt2 tokenizer\n",
    "            length: length of generated sequence.\n",
    "            device: torch.device object.\n",
    "            temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "    \"\"\"\n",
    "    \n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    generated = context\n",
    "    with torch.no_grad():  \n",
    "        for _ in range(length):\n",
    "            inputs = {'input_ids': generated}\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "    return generated\n",
    "\n",
    "\n",
    "def beam_search(model, context, length, beam_size, device, temperature=1):\n",
    "    \"\"\" Generate sequence using beam search https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    "        Args:\n",
    "            model: gpt/gpt2 model\n",
    "            context: tokenized text using gpt/gpt2 tokenizer\n",
    "            length: length of generated sequence.\n",
    "            beam_size: >=1 and <= total_no_of_tokens\n",
    "            device: torch.device object.\n",
    "            temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "    \"\"\"\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    with torch.no_grad():  \n",
    "        inputs = {'input_ids': context}\n",
    "        outputs = model(**inputs) \n",
    "        next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "        next_token_probs = F.softmax(next_token_logits)\n",
    "        scores, indices = torch.topk(next_token_probs, beam_size)\n",
    "        indices = indices.tolist()\n",
    "        sequences = [[c] for c in indices]\n",
    "        for _ in tnrange(length-1):\n",
    "            logits = torch.zeros(beam_size*len(next_token_logits))\n",
    "            for j in range(len(sequences)):\n",
    "                new_generated = torch.cat((context,torch.tensor([sequences[j]], dtype=torch.long, device=device)),dim=1)\n",
    "                inputs = {'input_ids': new_generated}\n",
    "                outputs = model(**inputs) \n",
    "                next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "                next_token_probs = F.softmax(next_token_logits)\n",
    "                start, stop = j*len(next_token_logits), (j+1)*len(next_token_logits)\n",
    "                logits[start:stop] = scores[j]*next_token_probs\n",
    "            scores, new_logits_indices = torch.topk(logits,beam_size)\n",
    "            logits = (new_logits_indices%50259).tolist()\n",
    "            for j in range(len(sequences)):\n",
    "                sequences[j] = sequences[j]+[logits[j]]\n",
    "    return scores, sequences\n",
    "\n",
    "\n",
    "def generate_beam_sample(data, tokenizer, model, num=1, length=100, beam_size=3, device=torch.device('cuda')):\n",
    "    \"\"\" Generate summaries for \"num\" number of articles using beam search.\n",
    "        Args:\n",
    "            data = GPT21024Dataset object\n",
    "            tokenizer = gpt/gpt2 tokenizer\n",
    "            num = number of articles for which summaries has to be generated\n",
    "    \"\"\"\n",
    "    for i in range(num):\n",
    "        sample = data[i]\n",
    "        idx = sample['sep_pos']\n",
    "        context = sample['article'][:idx].tolist()\n",
    "        summary = sample['article'][idx+1:][:100].tolist()\n",
    "        scores, sequences = beam_search(model, context, length, beam_size, device)\n",
    "        print('new_article', end='\\n\\n')\n",
    "        print(tokenizer.decode(context[:-1]), end='\\n\\n')\n",
    "        print('actual_summary', end='\\n\\n')\n",
    "        print(tokenizer.decode(summary), end='\\n\\n')\n",
    "        for i in range(len(sequences)):\n",
    "            text = tokenizer.convert_ids_to_tokens(sequences[i],skip_special_tokens=True)\n",
    "            text = tokenizer.convert_tokens_to_string(text)  \n",
    "            print(\"generated_summary-{} and Score is {}.\".format(i+1, scores[i]), end='\\n\\n')\n",
    "            print(text, end='\\n\\n')\n",
    "\n",
    "\n",
    "def oldgenerate_sample(data, tokenizer, model, num=1, eval_step=False, length=20, temperature=1, top_k=10, top_p=0.5, device=torch.device('cuda')):\n",
    "    \"\"\" Generate summaries for \"num\" number of articles.\n",
    "        Args:\n",
    "            data = GPT21024Dataset object\n",
    "            tokenizer = gpt/gpt2 tokenizer\n",
    "            model = gpt/gpt2 model\n",
    "            num = number of articles for which summaries has to be generated\n",
    "            eval_step = can be True/False, checks generating during evaluation or not\n",
    "    \"\"\"\n",
    "    for i in range(num):\n",
    "        sample = data[i]\n",
    "        idx = sample['sep_pos']\n",
    "        context = sample['token_ids'][:idx].tolist()\n",
    "        summary = sample['token_ids'][idx+1:][:100].tolist()\n",
    "        generated_text = sample_seq(model, context, length, device, temperature, top_k, top_p)\n",
    "        generated_text = generated_text[0, len(context):].tolist()\n",
    "        text = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n",
    "        text = tokenizer.convert_tokens_to_string(text)\n",
    "        if eval_step==False:\n",
    "            print('new_article', end='\\n\\n')\n",
    "            print(tokenizer.decode(context), end='\\n\\n')\n",
    "            print(\"generated_summary\", end='\\n\\n')\n",
    "            print(text, end='\\n\\n')\n",
    "            print('actual_summary', end='\\n\\n')\n",
    "            print(tokenizer.decode(summary), end='\\n\\n')\n",
    "        else:\n",
    "            print(tokenizer.decode(context), end='\\n\\n')\n",
    "            print(\"generated_summary\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(data, tokenizer, model, num=1, length=20, temperature=1, top_k=10, top_p=0.5, device=torch.device('cuda')):\n",
    "    for i in range(num):\n",
    "        print(\"*\"*50)\n",
    "        sample = data[i]\n",
    "        idx = sample['sep_pos']\n",
    "        description = sample['token_ids'][:idx].tolist()\n",
    "        title = sample['token_ids'][idx+1:][:100].tolist()\n",
    "        generated_tokens = sample_seq(model, description, length, device, temperature, top_k, top_p)\n",
    "        generated_tokens = generated_tokens[0, len(description):].tolist()\n",
    "        generated_title = tokenizer.convert_ids_to_tokens(generated_tokens)#,skip_special_tokens=True)\n",
    "        generated_title = tokenizer.convert_tokens_to_string(generated_title)\n",
    "\n",
    "        print('Description:\\n', tokenizer.decode(description, skip_special_tokens=True))\n",
    "        print(\"\\nGENERATED title:\\n\", generated_title)\n",
    "        print('TRUE title:\\n', tokenizer.decode(title,skip_special_tokens=True),\"   \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_model = torch.load('./output/models/model.pt')\n",
    "new_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "new_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "new_model.load_state_dict(torch.load('./output/models/model.pt'))\n",
    "new_model.eval()\n",
    "\n",
    "new_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Description:\n",
      " Dawson Geophysical (NASDAQ:DWSN &#8211; Get Rating) crossed above its 200-day moving average during trading on Tuesday. The stock has a 200-day moving average of $1.73 and traded as high as $1.73. Dawson Geophysical shares last traded at $1.65, with a volume of\n",
      "\n",
      "GENERATED title:\n",
      "  $1.73. Dawson Geophysical shares last traded at $1.65, with a volume\n",
      "TRUE title:\n",
      " Dawson Geophysical (NASDAQ:DWSN) Shares Cross Above 200-Day Moving Average of $1.73    \n",
      "\n",
      "**************************************************\n",
      "Description:\n",
      " The Energy Department now joins the Federal Bureau of Investigation in saying the virus likely spread via a mishap at a Chinese laboratory\n",
      "\n",
      "GENERATED title:\n",
      " The Energy Department now joins the FederalThe Energy Department now joins the Federal Bureau of Investigation in\n",
      "TRUE title:\n",
      " FBI chief confirms Covid-19 originated from lab incident in Wuhan    \n",
      "\n",
      "**************************************************\n",
      "Description:\n",
      " REX American Resources Co. (NYSE:REX) Shares Bought by Axa S.A.\n",
      "\n",
      "GENERATED title:\n",
      " REX American Resources Co. (NYSE:REX) Shares Bought by Axa S.\n",
      "TRUE title:\n",
      " REX American Resources Co. (NYSE:REX) Shares Bought by Axa S.A.    \n",
      "\n",
      "**************************************************\n",
      "Description:\n",
      " Cannabis Sativa (CBDS) &#038; Its Peers Critical Review\n",
      "\n",
      "GENERATED title:\n",
      " \n",
      "\n",
      "Cannabis Sativa (CBDS) &#038; Its Pe\n",
      "TRUE title:\n",
      " Cannabis Sativa (CBDS) &#038; Its Peers Critical Review    \n",
      "\n",
      "**************************************************\n",
      "Description:\n",
      " UNLV defensive lineman Ryan Keeler had been dealing with nausea and had felt sick for at least seven days prior to his tragic death on Feb. 20... this according to a new police report, obtained by TMZ Sports. In the Las Vegas Metropolitan Police&hellip;\n",
      "\n",
      "GENERATED title:\n",
      " unLV defensive lineman Ryan KeUNLV defensive lineman Ryan Keeler had been dealingUN\n",
      "TRUE title:\n",
      " UNLV's Ryan Keeler Battled Nausea, Sickness In Days Before Death, Police Report States    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sample(test_dataset, tokenizer, new_model, num=5, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orignal_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "orignal_model.resize_token_embeddings(len(tokenizer))\n",
    "orignal_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Description:\n",
      " Dawson Geophysical (NASDAQ:DWSN &#8211; Get Rating) crossed above its 200-day moving average during trading on Tuesday. The stock has a 200-day moving average of $1.73 and traded as high as $1.73. Dawson Geophysical shares last traded at $1.65, with a volume of\n",
      "\n",
      "GENERATED title:\n",
      " <|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>  <|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>  <|pad|>\n",
      "TRUE title:\n",
      " Dawson Geophysical (NASDAQ:DWSN) Shares Cross Above 200-Day Moving Average of $1.73    \n",
      "\n",
      "**************************************************\n",
      "Description:\n",
      " The Energy Department now joins the Federal Bureau of Investigation in saying the virus likely spread via a mishap at a Chinese laboratory\n",
      "\n",
      "GENERATED title:\n",
      " <|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "TRUE title:\n",
      " FBI chief confirms Covid-19 originated from lab incident in Wuhan    \n",
      "\n",
      "**************************************************\n",
      "Description:\n",
      " REX American Resources Co. (NYSE:REX) Shares Bought by Axa S.A.\n",
      "\n",
      "GENERATED title:\n",
      " <|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "TRUE title:\n",
      " REX American Resources Co. (NYSE:REX) Shares Bought by Axa S.A.    \n",
      "\n",
      "**************************************************\n",
      "Description:\n",
      " Cannabis Sativa (CBDS) &#038; Its Peers Critical Review\n",
      "\n",
      "GENERATED title:\n",
      " <|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "TRUE title:\n",
      " Cannabis Sativa (CBDS) &#038; Its Peers Critical Review    \n",
      "\n",
      "**************************************************\n",
      "Description:\n",
      " UNLV defensive lineman Ryan Keeler had been dealing with nausea and had felt sick for at least seven days prior to his tragic death on Feb. 20... this according to a new police report, obtained by TMZ Sports. In the Las Vegas Metropolitan Police&hellip;\n",
      "\n",
      "GENERATED title:\n",
      " <|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
      "TRUE title:\n",
      " UNLV's Ryan Keeler Battled Nausea, Sickness In Days Before Death, Police Report States    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sample(test_dataset, tokenizer, orignal_model, num=5, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
