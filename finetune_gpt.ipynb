{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Modify path so that we can import local modules into notebook\n",
    "module_path = os.path.abspath(os.path.join('./utils')) \n",
    "sys.path.insert(0, module_path)\n",
    "\n",
    "from utils.exp_util import load_config, init_model, get_tokenizer, setup_exp_folders\n",
    "from utils.data_util import get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GPT_SIZE': 'small', 'TARGET_TYPE': 'desc', 'TRAIN_SPLIT': 0.75, 'VAL_SPLIT': 0.15, 'TEST_SPLIT': 0.1, 'EPOCHS': 1, 'BATCH_SIZE': 8, 'LR': 5e-05, 'WARMUP_STEP': 100, 'GRADIENT_ACCUMULATION_STEPS': 32, 'MAX_GRAD_NORM': 1, 'RANDOM_SEED': 42}\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"small_test\"\n",
    "config = load_config(EXPERIMENT_NAME)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = config[\"RANDOM_SEED\"]\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.random.manual_seed(RANDOM_SEED)\n",
    "import random\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 articles loaded.\n",
      "6461 samples after cleaning\n",
      "Train: 4845, Val: 970, Test: 646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>description_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>freenet (FRA:FNTN) PT Set at EUR27.00 by Deuts...</td>\n",
       "      <td>freenet (FRA:FNTN - Get Rating) has been given...</td>\n",
       "      <td>[f, reen, et, Ġ(, F, RA, :, F, NT, N, ), ĠPT, ...</td>\n",
       "      <td>[f, reen, et, Ġ(, F, RA, :, F, NT, N, Ġ-, ĠGet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>Ally Financial (NYSE:ALLY) Upgraded to \"Hold\" ...</td>\n",
       "      <td>Ally Financial (NYSE:ALLY - Get Rating) was up...</td>\n",
       "      <td>[All, y, ĠFinancial, Ġ(, NYSE, :, ALLY, ), ĠUp...</td>\n",
       "      <td>[All, y, ĠFinancial, Ġ(, NYSE, :, ALLY, Ġ-, ĠG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>Baytex/Ranger Oil Combination First Of Its Kin...</td>\n",
       "      <td>Baytex Energy Corp. announced on Tuesday that ...</td>\n",
       "      <td>[Bay, tex, /, R, anger, ĠOil, ĠComb, ination, ...</td>\n",
       "      <td>[Bay, tex, ĠEnergy, ĠCorp, ., Ġannounced, Ġon,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>Aaron Rodgers, QBs become top attractions at N...</td>\n",
       "      <td>Several teams including the Packers will have ...</td>\n",
       "      <td>[Aaron, ĠRodgers, ,, ĠQB, s, Ġbecome, Ġtop, Ġa...</td>\n",
       "      <td>[Several, Ġteams, Ġincluding, Ġthe, ĠPackers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>PACAF Airmen assist in ENCAP during Cobra Gold...</td>\n",
       "      <td>U.S. Air Force Airmen from Joint Base Elmendor...</td>\n",
       "      <td>[PAC, AF, ĠA, irm, en, Ġassist, Ġin, ĠE, NC, A...</td>\n",
       "      <td>[U, ., S, ., ĠAir, ĠForce, ĠA, irm, en, Ġfrom,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "1703  freenet (FRA:FNTN) PT Set at EUR27.00 by Deuts...   \n",
       "3238  Ally Financial (NYSE:ALLY) Upgraded to \"Hold\" ...   \n",
       "5840  Baytex/Ranger Oil Combination First Of Its Kin...   \n",
       "5820  Aaron Rodgers, QBs become top attractions at N...   \n",
       "782   PACAF Airmen assist in ENCAP during Cobra Gold...   \n",
       "\n",
       "                                            description  \\\n",
       "1703  freenet (FRA:FNTN - Get Rating) has been given...   \n",
       "3238  Ally Financial (NYSE:ALLY - Get Rating) was up...   \n",
       "5840  Baytex Energy Corp. announced on Tuesday that ...   \n",
       "5820  Several teams including the Packers will have ...   \n",
       "782   U.S. Air Force Airmen from Joint Base Elmendor...   \n",
       "\n",
       "                                           title_tokens  \\\n",
       "1703  [f, reen, et, Ġ(, F, RA, :, F, NT, N, ), ĠPT, ...   \n",
       "3238  [All, y, ĠFinancial, Ġ(, NYSE, :, ALLY, ), ĠUp...   \n",
       "5840  [Bay, tex, /, R, anger, ĠOil, ĠComb, ination, ...   \n",
       "5820  [Aaron, ĠRodgers, ,, ĠQB, s, Ġbecome, Ġtop, Ġa...   \n",
       "782   [PAC, AF, ĠA, irm, en, Ġassist, Ġin, ĠE, NC, A...   \n",
       "\n",
       "                                     description_tokens  \n",
       "1703  [f, reen, et, Ġ(, F, RA, :, F, NT, N, Ġ-, ĠGet...  \n",
       "3238  [All, y, ĠFinancial, Ġ(, NYSE, :, ALLY, Ġ-, ĠG...  \n",
       "5840  [Bay, tex, ĠEnergy, ĠCorp, ., Ġannounced, Ġon,...  \n",
       "5820  [Several, Ġteams, Ġincluding, Ġthe, ĠPackers, ...  \n",
       "782   [U, ., S, ., ĠAir, ĠForce, ĠA, irm, en, Ġfrom,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(config[\"GPT_SIZE\"])\n",
    "train_dataset, val_dataset, test_dataset = get_datasets(config, tokenizer)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "train_dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example context:  Robert Platt honored as first in school history to bring home state title in boys wrestling\n",
      "Example target:  Brawley wrestling has been around since 1968 - and in 55 years of existence, the school has never had a boy state champ, until now.The post Robert Platt honored as first in school history to bring home state title in boys wrestling appeared first on KYMA.<|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = train_dataset[0]['token_ids']\n",
    "sep_idx =  train_dataset[0]['sep_pos']\n",
    "print(\"Example context: \", tokenizer.decode(sample_tokens[:sep_idx]))\n",
    "print(\"Example target: \", tokenizer.decode(sample_tokens[sep_idx+1:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, train_dataset, valid_dataset, ignore_index, config, checkpoint_every=0, checkpoint_path=None):\n",
    "    log_dir, model_dir = setup_exp_folders(EXPERIMENT_NAME)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=config[\"BATCH_SIZE\"])#,num_workers=args.num_workers)\n",
    "    EPOCHS = config[\"EPOCHS\"]\n",
    "    GRADIENT_ACCUMULATION_STEPS = config[\"GRADIENT_ACCUMULATION_STEPS\"]\n",
    "    \n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "    optimizer = AdamW(model.parameters(),lr=config[\"LR\"])\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=config[\"WARMUP_STEP\"], num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    start_epoch = 1\n",
    "\n",
    "    if checkpoint_path:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        loss = checkpoint[\"loss\"]\n",
    "        tr_loss = checkpoint[\"tr_loss\"]\n",
    "\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    for epoch in range(start_epoch, EPOCHS+1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            inputs, labels = batch['token_ids'], batch['token_ids']\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(inputs)[0]\n",
    "            idx = batch['sep_pos']\n",
    "\n",
    "            losses = []\n",
    "            for i, sep_idx in enumerate(idx):\n",
    "                shift_logits = logits[i, sep_idx:-1, :].contiguous()\n",
    "                shift_labels = labels[i, sep_idx+1:].contiguous()\n",
    "                l = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                losses.append(l)\n",
    "\n",
    "            # Combine the losses\n",
    "            loss = torch.stack(losses).mean()\n",
    "            loss = loss/GRADIENT_ACCUMULATION_STEPS\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"MAX_GRAD_NORM\"])\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                writer.add_scalar('loss', (tr_loss - logging_loss)/GRADIENT_ACCUMULATION_STEPS, global_step)\n",
    "                logging_loss = tr_loss\n",
    "\n",
    "            if (step + 1) % (10*GRADIENT_ACCUMULATION_STEPS) == 0:\n",
    "                results = evaluate(model, valid_dataset, config[\"BATCH_SIZE\"], ignore_index)\n",
    "                for key, value in results.items():\n",
    "                    writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "                model.train()\n",
    "\n",
    "        if epoch % checkpoint_every == 0:\n",
    "            # Save checkpoint\n",
    "            model_path = os.path.join(model_dir, f\"checkpoint_epoch{epoch}.pt\")\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'global_step': global_step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    'tr_loss': tr_loss\n",
    "                    }, model_path)\n",
    "            \n",
    "            print(f\"Saved checkpoint to {model_path}\\n\")\n",
    "\n",
    "    if epoch % checkpoint_every != 0:\n",
    "        # Save final checkpoint (if it wasn't already saved)\n",
    "        model_path = os.path.join(model_dir, f\"checkpoint_epoch{epoch}.pt\")\n",
    "        torch.save({\n",
    "                'epoch': EPOCHS,\n",
    "                'global_step': global_step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'tr_loss': tr_loss\n",
    "                }, model_path)\n",
    "        print(f\"Saved final checkpoint to {model_path}\\n\")\n",
    "    print(f\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataset, batch_size, ignore_index):\n",
    "    \"\"\" Returns perplexity score on validation dataset.\n",
    "        global_step: no. of times gradients have backpropagated\n",
    "        ignore_index: token not considered in loss calculation\n",
    "    \"\"\"\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        inputs, labels = batch['token_ids'].to(device), batch['token_ids'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)[0]\n",
    "            idx = batch['sep_pos']\n",
    "\n",
    "            losses = []\n",
    "            for i, sep_idx in enumerate(idx):\n",
    "                shift_logits = logits[i, sep_idx:-1, :].contiguous()\n",
    "                shift_labels = labels[i, sep_idx+1:].contiguous()\n",
    "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                losses.append(loss)\n",
    "\n",
    "            # Combine the losses\n",
    "            eval_loss += torch.stack(losses).mean()\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    return {\"perplexity\": perplexity, \"loss\": eval_loss}           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = init_model(tokenizer, config[\"GPT_SIZE\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dataset, val_dataset, tokenizer.pad_token_id, config, checkpoint_every=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
